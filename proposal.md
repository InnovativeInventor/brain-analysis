## Leading Question 
What types of compile-time performance optimizations can be done to Lisp source code, and, in particular, what is the real-world performance speedup obtained by various combinations of optimizations?

## Dataset Acquisition
We will be using the Common Lisp ansi-test test suite as our dataset. It contains a large amount of samples of Lisp code for us to interpret. The dataset is not too large - it contains 6.6 MB of Lisp source code. We will use a subset of the dataset, as we are not seeking to implement a totally compliant Common Lisp interpreter. We will use a subset containing only keywords we plan to implement in our interpreter (a very small subset of what is available in Common Lisp).

## Data format
The tests in the ansi-test test suite are broken down into categories (for example, numbers, arrays, cons). In each category, there is a list of .lsp Lisp files. Each .lsp file contains several deftest declarations defining a test. Some files contain sections for “error” tests and “non-error” tests. The “error” tests don’t concern us, as they test fairly specific features of Common Lisp. The “non-error” tests contain more interesting and Lisp-agnostic code.

## Data correction
We most likely will not fully adhere to the Common Lisp specification, as we will not implement all the language features. As a result, we will filter for tests that only use primitives we’ve implemented in our interpreter. So, we will (once we have thoroughly tested our parser) throw out tests that our parser does not support (for example, we may omit hash tables, quotes, and lists in our initial implementations).

## Data storage
Since our dataset is not very large, we can afford to store it as plain text. After all, the first stage of our interpreter will receive text source code as input. However, the parser transforms the textual input into an abstract syntax tree (AST), which will be optimized and executed by the interpreter. The AST will have a node for every language construct in the input code. The AST does not scale directly with the number of characters in the input file - for example, parentheses add no extra overhead to the AST size, as these are encoded into the tree structure (redundant parentheses are removed), and extra spaces are filtered out by the parser. As a result, the size of the AST depends on the number of language constructs in the input code (numbers, identifiers, keywords, etc). If the number of language constructs in our input file is N, the size of our internal representation of the AST is O(N), as we must allocate one node (which are, individually, constant sized) per language construct.

## Algorithm 
We will use various graph algorithms to process our input AST, and transform it into a logically equivalent AST that can be executed faster by the interpreter. We will execute the AST by doing a post-order depth-first traversal.

### Common Subexpression Elimination
The input to CSE is a control flow graph (CFG). This is a graph of basic blocks, where blocks are nominally separated by branches. We will need to preprocess the AST to transform it into a CFG (sequences of intermediate branchless instructions are each turned into individual basic blocks, basic blocks make available all subexpressions as temporaries that can be used in future basic blocks, branches causes divergences and eventual reconvergences in CFG). The output of CSE will also be a CFG, which we’ll need to convert back into an AST. For performing CSE, we’ll likely use Global Value Numbering (GVN). There are several algorithms for implementing GVN - for our implementation, we’d like to target O(n^3*j) runtime, where n is the number of SSA statements in the CFG, and j is the number of convergences that occur (basic blocks with more than one predecessor). We’d also like to target O(n) complexity, using hashing to store available expressions.

### Graph Coloring
We will use graph coloring to save on stack space usage of temporary values. The input to graph coloring is a graph to be colored - this wil not be our AST. We will first need to convert our AST into a CFG. Then, we will analyze the liveness of each temporary variable - that is, we will determine during which sections of the CFG graph our temporary variable is used. We will then create a graph where nodes represent temporaries, and edges are placed between temporaries that have overlapping periods of liveness. The output of this algorithm will be a coloring for each temporary - these colors will be used to determine which memory locations on the stack each variable will use. Ideally, variables that do not have to be in memory at the same time can occupy the same memory, saving space. Complete optimality of the coloring is not essential, so we will likely use the greedy coloring algorithm. We’d like to target O(n) runtime and O(n) space complexity.

## Timeline
The first step of our project will be implementing the parser. While we’d like to get a minimal version working by March 30th, the parser will constantly evolve as we add new features to the interpreter - thus, the parser completed by March 30th will be minimally viable, not complete. We will create the interpreter before any optimizations, as the interpreter will be necessary to validate that optimizations don’t change the behavior of the executed code. We’d like to have a completed interpreter finished by April 8th. By this time, we’d also like to have performed data correction, so that we can run our interpreter on the dataset. The CSE optimization will be the most difficult part to implement, so we hope to have it finished by April 22nd. The graph coloring optimization will be far simpler to implement, and we will have it finished by April 25th. This gives us ~2 weeks of wiggle room for testing, writing the final report, and making a presentation before May 6th.
